{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grabs dangerous information on road closures through NewsAPI.'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Grabs dangerous information on road closures through NewsAPI.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import InfoAPI\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "key = InfoAPI.NewsAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(city_name='Austin'):\n",
    "    \"\"\"Engine to gather all texts from different news and blogs for road closure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    city_name : string\n",
    "        Contains the city that you are in; if none given, default is Austin.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    texts : list\n",
    "        Contains a list of all texts that have road closure in it.\n",
    "    \n",
    "    \"\"\"\n",
    "    date = datetime.now()\n",
    "    date_format = f\"{date.year}-{date.month}-{date.day}\"\n",
    "    \n",
    "    BASE_URL = \"https://newsapi.org/v2/everything/\"\n",
    "\n",
    "    params = {\n",
    "        'q': city_name,\n",
    "        'apiKey': key.API_KEY,\n",
    "        'from': date_format,\n",
    "        'to': date_format,\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params)\n",
    "\n",
    "    articles = response.json()['articles']\n",
    "    urls = get_urls(articles)\n",
    "    texts = use_urls(urls)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(articles):\n",
    "    \"\"\"Simply extracts the urls.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    articles : list\n",
    "        List of objects that contain an attribute url.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    urls : list\n",
    "        List of extracted urls.\n",
    "    \n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for article in articles:\n",
    "        urls.append(article['url'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_urls(urls):\n",
    "    \"\"\"Extracts text from each website.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    urls : list\n",
    "        A list of urls.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    texts : list\n",
    "        A list of extracted texts from given urls.\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    texts = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            text = get_texts(response)\n",
    "            if len(text) != 0:\n",
    "                texts.append(text)\n",
    "        else:\n",
    "            count = count + 1\n",
    "    print(f\"There was {count} error(s).\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(response):\n",
    "    \"\"\"Extracts the text from the p element.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    response : Response object\n",
    "        e.g. response = requests.get(url)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    texts : list\n",
    "    \n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    texts = []\n",
    "    for p in paragraphs:\n",
    "        text = p.text\n",
    "        text = process(text)\n",
    "        if text != '':\n",
    "            texts.append(text)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    \"\"\"Grabs everything after road closure, inclusively.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    text : string\n",
    "        A single string of text.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matches[0] : string\n",
    "        If there is a match, then it will return the string begining with road closure.\n",
    "    '' : string\n",
    "        Returns empty string if there is not a match.\n",
    "    \n",
    "    \"\"\"\n",
    "    regex = r\"road closure .*\"\n",
    "    matches = re.search(regex, text, re.MULTILINE | re.IGNORECASE)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
